{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1481\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'c:\\\\Users\\\\amari\\\\Desktop\\\\Code\\\\Rutgers\\\\MACHINEVISION\\\\myenv\\\\Lib\\\\site-packages\\\\numpy\\\\random'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 305\u001b[0m\n\u001b[0;32m    302\u001b[0m right_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(right_image_path)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Perform segmentation on both stereo images\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m left_masks, left_confidences, left_class_ids \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_objects_with_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m right_masks, right_confidences, right_class_ids \u001b[38;5;241m=\u001b[39m detect_objects_with_masks(right_image, model, output_dir, target_class_id, conf_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# Apply masks to both images\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m, in \u001b[0;36mdetect_objects_with_masks\u001b[1;34m(image, model, output_dir, target_class_id, conf_threshold)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_objects_with_masks\u001b[39m(image, model, output_dir, target_class_id, conf_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     masks, confidences, class_ids \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result_idx, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# Process each result\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:179\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    152\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    153\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:557\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:239\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Warmup model\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_warmup:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriton\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_warmup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, [], \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:729\u001b[0m, in \u001b[0;36mAutoBackend.warmup\u001b[1;34m(self, imgsz)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwarmup\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgsz\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m640\u001b[39m)):\n\u001b[0;32m    723\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;124;03m    Warm up the model by running one forward pass with a dummy input.\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \n\u001b[0;32m    726\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m        imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m  \u001b[38;5;66;03m# noqa (import here so torchvision import time not recorded in postprocess time)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     warmup_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monnx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriton, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(warmup_types) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriton):\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torchvision\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torchvision\\models\\convnext.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torchvision\\ops\\__init__.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgiou_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generalized_box_iou_loss\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoolers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_align\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ps_roi_align, PSRoIAlign\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ps_roi_pool, PSRoIPool\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torchvision\\ops\\poolers.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboxes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m box_area\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroi_align\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roi_align\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# is not supported by ONNX tracing yet.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# that merges the levels to the right indices\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39munused\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_onnx_merge_levels\u001b[39m(levels: Tensor, unmerged_results: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torchvision\\ops\\roi_align.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_compile_supported\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BroadcastingList2\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pair\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileTimeInstructionCounter\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\torch\\_dynamo\\utils.py:101\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# NOTE: Make sure `NP_SUPPORTED_MODULES` and `NP_TO_TNP_MODULE` are in sync.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np:\n\u001b[0;32m     97\u001b[0m     NP_SUPPORTED_MODULES: Tuple[types\u001b[38;5;241m.\u001b[39mModuleType, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     98\u001b[0m         np,\n\u001b[0;32m     99\u001b[0m         np\u001b[38;5;241m.\u001b[39mfft,\n\u001b[0;32m    100\u001b[0m         np\u001b[38;5;241m.\u001b[39mlinalg,\n\u001b[1;32m--> 101\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m,\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    104\u001b[0m     NP_TO_TNP_MODULE \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    105\u001b[0m         np: tnp,\n\u001b[0;32m    106\u001b[0m         np\u001b[38;5;241m.\u001b[39mfft: tnp\u001b[38;5;241m.\u001b[39mfft,\n\u001b[0;32m    107\u001b[0m         np\u001b[38;5;241m.\u001b[39mlinalg: tnp\u001b[38;5;241m.\u001b[39mlinalg,\n\u001b[0;32m    108\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom: tnp\u001b[38;5;241m.\u001b[39mrandom,\n\u001b[0;32m    109\u001b[0m     }\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\numpy\\__init__.py:354\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dtypes\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolynomial\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\amari\\Desktop\\Code\\Rutgers\\MACHINEVISION\\myenv\\Lib\\site-packages\\numpy\\random\\__init__.py:180\u001b[0m\n\u001b[0;32m    126\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinomial\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzipf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    177\u001b[0m ]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pickle\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _common\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _bounded_integers\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1524\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1496\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1483\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1459\u001b[0m, in \u001b[0;36m_path_hooks\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen zipimport>:75\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLO Model\n",
    "# Load YOLOv9 Segmentation Model\n",
    "def load_yolo_segmentation_model(model_path):\n",
    "    return YOLO(model_path)\n",
    "\n",
    "# Perform Object Detection and Segmentation\n",
    "def detect_objects_with_masks(image, model, output_dir, target_class_id, conf_threshold=0.5):\n",
    "    results = model(image)  # Perform inference\n",
    "    masks, confidences, class_ids = [], [], []\n",
    "\n",
    "    for result_idx, result in enumerate(results):\n",
    "        # Process each result\n",
    "        for obj_idx, (mask, conf, cls_id) in enumerate(zip(result.masks.data, result.boxes.conf, result.boxes.cls)):\n",
    "            if conf > conf_threshold:  # Apply confidence threshold\n",
    "                binary_mask = (mask > 0.5).cpu().numpy().astype(np.uint8)  # Threshold and convert to binary\n",
    "                mask_path = os.path.join(output_dir, f\"mask_result{result_idx}_obj{obj_idx}_class{int(cls_id)}.png\")\n",
    "                cv2.imwrite(mask_path, binary_mask * 255)  # Save mask as an image\n",
    "                #print(f\"Saved mask to {mask_path}\")\n",
    "                \n",
    "                # Append data to lists for further use\n",
    "                masks.append(binary_mask)\n",
    "                confidences.append(conf)\n",
    "                class_ids.append(int(cls_id))\n",
    "\n",
    "    print(type(result.masks.data))  # Debugging output\n",
    "    print(result.masks.data.shape)  # Debugging output\n",
    "\n",
    "    return masks, confidences, class_ids\n",
    "\n",
    "# Apply Segmentation Masks to Image and Save\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def apply_masks_to_image(image, masks, output_directory, imageside):\n",
    "    # Get dimensions of the image\n",
    "    height, width = image.shape[:2]  \n",
    "    filename_prefix = f\"{imageside}image_with_mask_\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # List to hold all the masked images\n",
    "    masked_images = []\n",
    "\n",
    "    # Apply each mask to create separate images\n",
    "    for i, mask in enumerate(masks):\n",
    "        # Resize the mask to match the image dimensions\n",
    "        resized_mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Ensure the mask is binary (0 or 1)\n",
    "        binary_mask = (resized_mask > 0).astype(np.uint8)\n",
    "\n",
    "        # Create a black image with the same dimensions as the original\n",
    "        masked_image = np.zeros_like(image)\n",
    "        \n",
    "        # Apply the mask to the original image (only keep the masked area)\n",
    "        masked_image[binary_mask == 1] = image[binary_mask == 1]  # Only keep the masked area\n",
    "\n",
    "        # Save the separate image with only the masked area visible\n",
    "        output_path = os.path.join(output_directory, f\"{filename_prefix}{i+1}.png\")\n",
    "        cv2.imwrite(output_path, masked_image)\n",
    "        #print(f\"Saved masked image: {output_path}\")\n",
    "        \n",
    "        # Append the masked image to the list\n",
    "        masked_images.append(masked_image)\n",
    "\n",
    "    # Return the list of all masked images\n",
    "    return masked_images\n",
    "\n",
    "# Save the Image with Masks Applied\n",
    "def save_image_with_masks(image_with_masks, output_path):\n",
    "    cv2.imwrite(output_path, image_with_masks)\n",
    "    #print(f\"Saved image with masks to {output_path}\")\n",
    "\n",
    "# Save Segmentation Masks as Separate Files (if needed)\n",
    "def save_segmentation_masks(masks, output_dir, filename_prefix=\"mask\"):\n",
    "    mask_paths = []\n",
    "    for i, mask in enumerate(masks):\n",
    "        mask_path = os.path.join(output_dir, f\"{filename_prefix}_{i}.png\")\n",
    "        cv2.imwrite(mask_path, mask * 255)  # Masks are 0 or 1, so multiply by 255 to visualize as white/black\n",
    "        mask_paths.append(mask_path)\n",
    "        #print(f\"Saved mask to {mask_path}\")\n",
    "    \n",
    "    return mask_paths\n",
    "\n",
    "# Stereo Image Disparity Calculation (optional)\n",
    "def calculate_disparity_map(left_image, right_image):\n",
    "    # Convert to grayscale for disparity calculation\n",
    "    left_gray = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    right_gray = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use stereo block matching to compute disparity map\n",
    "    stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)\n",
    "    disparity = stereo.compute(left_gray, right_gray)\n",
    "    \n",
    "    # Normalize the disparity map for visualization\n",
    "    disparity_normalized = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    disparity_normalized = np.uint8(disparity_normalized)\n",
    "    \n",
    "    return disparity_normalized\n",
    "\n",
    "# Draw YOLO Bounding Boxes\n",
    "def draw_bounding_boxes(image, boxes, output_path):\n",
    "    for i, (x, y, w, h) in enumerate(boxes):\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"Object {i}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    cv2.imwrite(output_path, image)\n",
    "\n",
    "# Detect and Show Keypoints Using SIFT\n",
    "def detect_and_show_keypoints(image, detector, output_dir, filename_prefix=\"keypoints\"):\n",
    "    if image is None or image.size == 0:\n",
    "        print(\"Error: Empty image passed to keypoint detection.\")\n",
    "        return [], None\n",
    "\n",
    "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints, descriptors = detector.detectAndCompute(grayscale_image, None)\n",
    "\n",
    "    keypoint_image = cv2.drawKeypoints(\n",
    "        image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "    )\n",
    "\n",
    "    keypoint_image_path = os.path.join(output_dir, f\"{filename_prefix}.jpg\")\n",
    "    cv2.imwrite(keypoint_image_path, keypoint_image)\n",
    "    #print(f\"Keypoint detection saved to {keypoint_image_path}\")\n",
    "\n",
    "    return keypoints, descriptors\n",
    "\n",
    "\n",
    "# Match Keypoints with BFMatcher\n",
    "def match_keypoints_sift_with_bf(image1, image2, keypoints1, descriptors1, keypoints2, descriptors2, distance_threshold=50.0):\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    good_matches = [m for m in matches if m.distance < 0.8 * np.mean([m.distance for m in matches])]\n",
    "    filtered_matches = []\n",
    "\n",
    "    for match in good_matches:\n",
    "        pt1 = keypoints1[match.queryIdx].pt\n",
    "        pt2 = keypoints2[match.trainIdx].pt\n",
    "        if np.linalg.norm(np.array(pt1) - np.array(pt2)) < distance_threshold:\n",
    "            filtered_matches.append(match)\n",
    "\n",
    "    return filtered_matches\n",
    "\n",
    "\n",
    "# Process Stereo Images and Match Sub-Images with Duplicate Check\n",
    "def process_stereo_images_with_masked_objects(left_images, right_images, output_dir):\n",
    "    img1u = []\n",
    "    img1v = []\n",
    "    img2u = []\n",
    "    img2v = []\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    # SIFT detector setup\n",
    "    sift = cv2.SIFT_create(nfeatures=12800)\n",
    "\n",
    "    # Process each pair of left and right images\n",
    "    for left_image, right_image in zip(left_images, right_images):\n",
    "        # Ensure that the images are numpy arrays\n",
    "        if not isinstance(left_image, np.ndarray) or not isinstance(right_image, np.ndarray):\n",
    "            print(\"Error: Expected numpy.ndarray images\")\n",
    "            continue\n",
    "\n",
    "        matched_left_indices = set()\n",
    "        matched_right_indices = set()\n",
    "\n",
    "        # Process detected objects from both images\n",
    "        for i, left_image in enumerate(left_images):\n",
    "            for j, right_image in enumerate(right_images):\n",
    "                # Skip if either of the boxes has already been matched\n",
    "                if i in matched_left_indices or j in matched_right_indices:\n",
    "                    continue\n",
    "\n",
    "                # Detect keypoints and descriptors for both images\n",
    "                keypoints1, descriptors1 = detect_and_show_keypoints(left_image, sift, output_dir, f\"keypoints_left_{i}_{j}\")\n",
    "                keypoints2, descriptors2 = detect_and_show_keypoints(right_image, sift, output_dir, f\"keypoints_right_{i}_{j}\")\n",
    "\n",
    "                if not keypoints1 or not keypoints2:\n",
    "                    continue\n",
    "\n",
    "                # Match keypoints between the two images using BFMatcher\n",
    "                filtered_matches = match_keypoints_sift_with_bf(left_image, right_image, keypoints1, descriptors1, keypoints2, descriptors2)\n",
    "\n",
    "                if len(filtered_matches) > 4:\n",
    "                    # Save the matching keypoints visualization\n",
    "                    img_matches = cv2.drawMatches(left_image, keypoints1, right_image, keypoints2, filtered_matches, None, flags=2)\n",
    "                    match_path = os.path.join(output_dir, f\"matches_{i}_{j}.jpg\")\n",
    "                    #print(f\"Saving match visualization: {match_path}\")\n",
    "                    cv2.imwrite(match_path, img_matches)\n",
    "                    \n",
    "                    img1u = []\n",
    "                    img1v = []\n",
    "                    img2u = []\n",
    "                    img2v = []\n",
    "                    # Log the matched keypoints' coordinates\n",
    "                    for match in filtered_matches:\n",
    "                        pt1 = keypoints1[match.queryIdx].pt\n",
    "                        pt2 = keypoints2[match.trainIdx].pt\n",
    "                        img1u.append(pt1[0])  # x-coordinate of image 1\n",
    "                        img1v.append(pt1[1])  # y-coordinate of image 1\n",
    "                        img2u.append(pt2[0])  # x-coordinate of image 2\n",
    "                        img2v.append(pt2[1])  # y-coordinate of image 2\n",
    "                    build_3D_cloud(img1u, img1v, img2u, img2v)\n",
    "\n",
    "                    all_matches.append((len(filtered_matches), i, j, filtered_matches, keypoints1, keypoints2))\n",
    "\n",
    "                    # Mark the indices as matched to prevent further matching\n",
    "                    matched_left_indices.add(i)\n",
    "                    matched_right_indices.add(j)\n",
    "\n",
    "def build_3D_cloud(img1u, img1v, img2u, img2v):\n",
    "    if len(img1u) == 0 or len(img2u) == 0:\n",
    "        print(\"No points provided for reconstruction.\")\n",
    "        return\n",
    "\n",
    "    # Intrinsic calibration matrix\n",
    "    K = np.array([\n",
    "        [9.25692841e+03, 0, 4.58239711e+02],\n",
    "        [0, 8.37883743e+04, 3.59148084e+02],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # Matched points from two views\n",
    "    # Assuming img1u, img1v, img2u, img2v are the matched point coordinates\n",
    "    img1_points = np.column_stack((img1u, img1v)).astype(np.float32)\n",
    "    img2_points = np.column_stack((img2u, img2v)).astype(np.float32)\n",
    "\n",
    "    # Normalize the 2D points\n",
    "    img1_norm = cv2.undistortPoints(img1_points, cameraMatrix=K, distCoeffs=None)\n",
    "    img2_norm = cv2.undistortPoints(img2_points, cameraMatrix=K, distCoeffs=None)\n",
    "\n",
    "    # Ensure the matrices are contiguous after undistortion and reshaping\n",
    "    img1_norm = img1_norm.reshape(-1, 2).copy()  # Ensure it's contiguous\n",
    "    img2_norm = img2_norm.reshape(-1, 2).copy()  # Ensure it's contiguous\n",
    "\n",
    "    print(img1_norm.shape)  # Should be (n, 2) for keypoint matches\n",
    "    print(img2_norm.shape)  # Should match the same dimensions\n",
    "\n",
    "    print(img1_norm.shape)  # Should be (n, 2) or (n, 3) for keypoint matches\n",
    "    print(img2_norm.shape)  # Should match the same dimensions\n",
    "\n",
    "    # Compute the essential matrix\n",
    "    E, mask = cv2.findEssentialMat(img1_norm, img2_norm, K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "\n",
    "    # Recover pose (R: rotation matrix, t: translation vector)\n",
    "    _, R, t, _ = cv2.recoverPose(E, img1_norm, img2_norm, K)\n",
    "    print(\"Rotation matrix R:\\n\", R)\n",
    "    print(\"Translation vector t:\\n\", t)\n",
    "    plt.scatter(img1u, img1v, label='Image 1 Points')\n",
    "    plt.scatter(img2u, img2v, label='Image 2 Points')\n",
    "    plt.legend()\n",
    "    plt.title(\"Matched Points\")\n",
    "    plt.show()\n",
    "\n",
    "    # Triangulate points\n",
    "    P1 = np.hstack((np.eye(3), np.zeros((3, 1))))  # First camera projection matrix\n",
    "    P2 = np.hstack((R, t))                        # Second camera projection matrix\n",
    "\n",
    "    points_4d_homogeneous = cv2.triangulatePoints(K @ P1, K @ P2, img1_norm.T, img2_norm.T)\n",
    "    points_3d = points_4d_homogeneous[:3] / points_4d_homogeneous[3]  # Normalize homogeneous coordinates\n",
    "\n",
    "    # Transpose to get points in (N, 3) format\n",
    "    points_3d = points_3d.T\n",
    "\n",
    "    # Plot the resulting 3D point cloud\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    sc = ax.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], c=points_3d[:, 2], cmap='viridis')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.colorbar(sc, label='Depth (Z)')\n",
    "    plt.title(\"3D Point Cloud with Depth\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Generated 3D points:\\n{points_3d[:5]} (showing first 5 points)\")\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    target_class_id = None\n",
    "    output_dir = \"ultralyticsoutput\"  # Directory for outputs\n",
    "    model_path = \"yolov9e-seg.pt\"  # Replace with your model path\n",
    "    model = load_yolo_segmentation_model(model_path)\n",
    "\n",
    "    # Load stereo images (left and right)\n",
    "    left_image_path = \"IMG_7998.jpg\"\n",
    "    right_image_path = \"IMG_7999.jpg\"\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "    \n",
    "    # Perform segmentation on both stereo images\n",
    "    left_masks, left_confidences, left_class_ids = detect_objects_with_masks(left_image, model, output_dir, target_class_id, conf_threshold=0.5)\n",
    "    right_masks, right_confidences, right_class_ids = detect_objects_with_masks(right_image, model, output_dir, target_class_id, conf_threshold=0.5)\n",
    "    \n",
    "    # Apply masks to both images\n",
    "    left_image_with_masks = apply_masks_to_image(left_image, left_masks, output_dir, imageside = \"left\")\n",
    "    right_image_with_masks = apply_masks_to_image(right_image, right_masks, output_dir, imageside = \"right\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "    \n",
    "    # Save each mask separately with unique names\n",
    "    for idx, mask in enumerate(left_masks):\n",
    "        mask_path = os.path.join(output_dir, f\"left_mask_{idx}.png\")\n",
    "        cv2.imwrite(mask_path, mask * 255)  # Save mask as binary image\n",
    "        print(f\"Saved left mask {idx} to {mask_path}\")\n",
    "    \n",
    "    for idx, mask in enumerate(right_masks):\n",
    "        mask_path = os.path.join(output_dir, f\"right_mask_{idx}.png\")\n",
    "        cv2.imwrite(mask_path, mask * 255)  # Save mask as binary image\n",
    "        print(f\"Saved right mask {idx} to {mask_path}\")\n",
    "    \n",
    "    # Optional: Calculate disparity map (depth estimation)\n",
    "    disparity_map = calculate_disparity_map(left_image, right_image)\n",
    "    \n",
    "    # Save or display the disparity map\n",
    "    disparity_map_output_path = os.path.join(output_dir, \"disparity_map.jpg\")\n",
    "    cv2.imwrite(disparity_map_output_path, disparity_map)\n",
    "    print(f\"Saved disparity map to {disparity_map_output_path}\")\n",
    "\n",
    "    process_stereo_images_with_masked_objects(left_image_with_masks, right_image_with_masks, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
